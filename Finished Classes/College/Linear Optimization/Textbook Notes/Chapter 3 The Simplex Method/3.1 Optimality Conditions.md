> **Definition 3.1** Let ${x}$ be an element of a polyhedron ${P}$. A vector ${d \in \Bbb{R}^n}$ is said to be a *feasible direction* at ${x}$ if there exists a positive scalar ${\theta}$ for which ${x + \theta d \in P}$.

Let ${x}$ be a basic feasible solution to the standard form problem.
Let ${B(1), ... B(m)}$ be the indices of the basic variables.
Let ${B = [A_{B(1)}...A_{B(m)}]}$ be the corresponding basis matrix.

We have ${x_i = 0}$ for every nonbasic variable, and the vector ${x_B = (x_{B(1)},...x_{B(m)})}$ of basic variables is 
$${x_B = B^{-1}b}$$

>[!info]- Consider
> Let's consider moving away from ${x}$ to a new vector ${x+\theta d}$ by selecting a nonbasic variable ${x_j}$ (initially at zero) and increasing it to a positive value ${\theta}$, keeping the remaining nonbasic variables at zero.
>
${d_j = 1}$ and ${d_i = 0}$ for every nonbasic index ${i}$ other than ${j}$. ^541973
>
The vector ${x_B}$ also changes to ${x_B + \theta d_B}$, where ${d_B = (d_{B(1)}, d_{B(2)}, ... d_{B(m)})}$ is a vector with components of ${d}$ corresponding to basic variables.
>
Since we are only interested in feasible regions, we take ${A(x + \theta d) = b}$.
Since ${x}$ is feasible, we know that ${Ax = b}$.
So for the inequality constraints to be satisfied for ${\theta > 0}$, we need ${Ad = 0}$.
>
Recall [[#^541973 | earlier]], ${d_j = 1}$ and ${d_i = 0}$ for every nonbasic index ${i}$ other than ${j}$.
>
Then,
$${0 = Ad = \sum\limits_{i=1}^nA_id_i = \sum\limits_{i=1}^m A_{B(i)}d_{B(i)} + A_j = Bd_B + A_j}$$

Since the basis matrix ${B}$ is invertible, we obtain
$${d_B = -B^{-1}A_j}$$ ^bcf4f2

This direction we have constructed will be called *${j}$th basic direction*.

Two cases:

*a)* Suppose that ${x}$ is a nondegenerate basic feasible solution. Then ${x_B > 0}$, from which it follows that ${x_B + \theta d_B \ge 0}$, and feasibility is maintained, when ${\theta}$ is sufficiently small. In particular, ${d}$ is a feasible direction.

*b)* Suppose now that ${x}$ is degenerate. Then ${d}$ is not always a feasible. It is possible that a basic variable ${x_{B(i)}}$ is zero, while that corresponding component ${d_{B(i)}}$ of ${d_B = -B^{-1}A_j}$ is negative. In that case, if we follow the ${j}$th basic direction, the nonnegativity constraint for ${x_{B(i)}}$ is immediately violated, and we are led to infeasible solutions; see Figure 3.2.

We now study the effects on the cost function if we move along a basic direction. If ${d}$ is the ${j}$th basic direction, then the rate ${c^Td}$ of cost change along the direction ${d}$ is given by ${c^T_Bd_B + c_j}$, where ${c_B = (c_{B(1)},...,c_{B(m)})}$.
Recall [[#^bcf4f2| earlier]], this is the same as 
$${c^T-c^T_BB^{-1}A_j}$$
Intuitively:
${c_j}$ is the cost per unit increase in variable ${x_j}$
${-c^T_BB^{-1}A_j}$ is the cost of compensating change in the basic variable necessitated by the constraint ${Ax = b}$.

>**Definition 3.2** Let ${x}$ be a basic solution, let ${B}$ be an associated basis matrix, and let ${c_B}$ be the vector of costs of the basic variables. For each ${j}$, we define the *reduced cost* ${\bar{c}_j}$ of the variable ${x_j}$ accordingly to the formula
>$${c_j = c_j-c^T_BB^{-1}A_j}$$

___

**Example 3.1** Consider the linear programming problem
$${\begin{align} \text{minimize }\;&c_1x_1+c_2x_2+c_3x_3+c_4x_4 \\ \text{subject to }\;&x_1 + x_2 + x_3 + x_4 = 2 \\ &2x_1 + 0x_2 + 3x_3 + 4x_4 = 2\\ &x_1,x_2,x_3,x_4 \ge 0\end{align}}$$

The first two columns of the matrix ${A}$ are ${A_1 = (1,2)}$ and ${A_2 = (1,0)}$.
Since these two are linearly independent, we can choose ${x_1}$ and ${x_2}$ as our basic variables.
The corresponding basis matrix is
$${B = \begin{bmatrix} 1 & 1 \\ 2 & 0 \end{bmatrix}}$$

We set ${x_3 = x_4 = 0}$ and solve for ${x_1, x_2}$, obtaining ${x_1 = 1, x_2 = 1}$. We have a *nondegenerate basic feasible solution

A basic direction corresponding to an increase in the nonbasic variable ${x_3}$, is constructed as follows. We have ${d_3 = 1}$ and ${d_4 = 0}$. The direction of change of the basic variables is
$${\begin{bmatrix} d_1 \\ d_2 \end{bmatrix} = \begin{bmatrix} d_{B(1)} \\ d_{B(2)} \end{bmatrix} = d_B = -B^{-1}A_3 = - \begin{bmatrix} 0 & \frac{1}{2} \\ 1 & -\frac{1}{2} \end{bmatrix} \begin{bmatrix} 1 \\ 3 \end{bmatrix} = \begin{bmatrix} -\frac{3}{2} \\ \frac{1}{2}\end{bmatrix}}$$
The cost of moving along this basic direction is ${c^Td = -\frac{3}{2}c_1 + \frac{1}{2}c_2 + c_3}$. 
This is the same as the *reduced cost* of the variable ${x_3}$.

___

Consider now *Definition 3.2* For the case of a basic variable. Since ${B}$ is the matrix ${\begin{bmatrix} A_{B(1)} ... A_{B(m)} \end{bmatrix}}$, we have ${B^{-1}\begin{bmatrix} A_{B(1)} ... A_{B(m)} \end{bmatrix} = I}$ where ${I}$ is the ${m \times m}$ identity matrix.

Particularly, ${B^{-1}A_{B(i)}}$ is the ${i}$th column of the identity matrix, which is the ${i}$th unit vector ${e_i}$. Therefore, for every basic variable ${x_{B(i)}}$, we have
$${\bar{c}_{B(i)} = c_{B(i)} - c^T_BB^{-1}A_{B(i)} = c_{B(i)} - c^T_Be_i = c_{B(i)} - c_{B(i)} = 0}$$

So the *reduced cost of every basic variable is zero*.

>**Theorem 3.1** Consider a basic feasible solution ${x}$ associated with a basis matrix ${B}$, and let ${\bar{c}}$ be the corresponding vector of reduced costs.
>*(a)* If ${\bar{c} \ge 0}$, then ${x}$ is optimal
>*(b)* If ${x}$ is optimal and nondegenerate, then ${\bar{c} \ge 0}$

*Proof.*
*(a)* We assume that ${\bar{c} \ge 0}$, we let ${y}$ be an arbitrary feasible solution, and we define ${y-x}$. Feasibility implies that ${Ax = Ay = b}$ and, therefore, ${Ad = 0}$. the latter equality can be rewritten in the form 
$${Bd_B + \sum\limits_{i \in N}A_id_i = 0}$$
where ${N}$ is the set of indices corresponding to the nonbasic variables under the given basis. Since ${B}$ is invertible, we obtain
$${d_B = -\sum\limits_{i \in N}B^{-1}A_id_i}$$
and
$${c^Td = c^T_Bd-B + \sum\limits_{i\in N}c_id_i = \sum\limits_{i\in N} (c_i - c_B^TB^{-1}A_i)d_i = \sum\limits_{i\in N}\bar{c}_id_i}$$