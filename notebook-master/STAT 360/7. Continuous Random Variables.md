Key: $P(X=x) = 0$. Probability mass is useless to us now!

>**Definition** Probability Density Function (pdf)
>The probability density function of $X$ is a function $f(x)$ such that for any two numbers $a$ and $b$ with $a \le b$
>$$\Pr(a \le X \le b) = \int_{a}^b f(x)dx$$
>That is, the probability that $X$ takes a value in the interval $[a,b]$ is the are above this interval and under the graph of density function


The graph $f(x)$ is often referred to as the *density curve*

Conditions for a pdf:
- $f(x) \ge 0$ for all $x$
- $\int_{-\infty}^\infty f(x)dx = 1$ (area under the curve $=$ 1)


>**Definition** Cumulative Distribution Function
> The cumulative distribution function (cdf) $F(X)$ for a continuous random variable $X$ is defined for every number $x$ by 
> $$F(x) = \Pr(X \le x) = \int_{-\infty}^x f(y)dy$$

___

### **Uniform Distribution**

 A continuous random variable $X$ is said to have a *uniform distribution* on the interval $[A,B]$ if the pdf of $X$ is

PDF:
$f(x; A,b) = \begin{cases} \frac{1}{B-A} & A \le x \le B \\ 0 & \text{otherwise} \end{cases}$

CDF:
$F(x) = \int_A^x \frac{1}{B-A}dy = \frac{1}{B-A}y \bigg|_{y=A}^{y=x} = \frac{x-A}{B-A}$

$E[X] = \int_{-\infty}^\infty x \cdot f(x) dx$

If $h(x)$ is a function of $X$, then
$$E[h(X)] = \int_{-\infty}^\infty h(x) \cdot f(x) dx$$

$\text{Var}(X) = E[(x-\mu)^2] = \int_{-\infty}^\infty (x-\mu)^2f(x)dx$ 

___

### **The Normal Distribution**

A continuous random variable $X$ is said to have a *normal distribution* with parameters $\mu$ and $\sigma$, if the pdf of $X$ is 
$$f(x; \mu, \sigma) = \frac{1}{\sigma\sqrt{2\pi}}e^{-\frac{1}{2}(\frac{x-\mu}{\sigma})^2}$$

If $X \sim N(\mu, \sigma)$, then $E(X) = \mu$ and $V(X) = \sigma^2$

![[7. Continuous Random Variables 2025-09-18 15.52.22.excalidraw]]

### **Standardizing the Normal Distribution**

Given a normal distribution $X \sim N(\mu, \sigma)$

We can standardize it to $Z \sim \frac{X - \mu}{\sigma}$