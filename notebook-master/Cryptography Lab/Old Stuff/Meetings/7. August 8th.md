- Goldreich Levin Theorem
- Pseudorandomness
- Expander graphs and Random Walks
	- We can use a little more bits than log(n) (as in Goldreich Levin) and allow the amplification (used Chernoff bound in Goldreich Levin) to happen faster
- Randomness extraction
	- Goldreich Levin is a randomness extraction
- Error correcting code
	- The $gl(x,r)\oplus gl(x,r \oplus e^i)$ is an example
- PRG revisited

Things to think about

- When thinking about probability, always consider *what* the probability is over

___

## **Homework**

*Come up with a concise way to show Goldreich-Levin Full Proof*

Suppose
$$\Pr\limits_{x,r\in \{0,1\}^n}[A(f(x),r) = gl(x,r)]\ge 1/2 + \epsilon(n)$$
for some non-negligible $\epsilon(n)$.

We can construct reduction $B$
1. Set 
$$
\ell = \left\lceil \log\left(\frac{2n}{\varepsilon(n)^2} + 1\right) \right\rceil
$$

2. Choose independent uniform 
$$
s_1, \dots, s_\ell \in \{0,1\}^n \quad \text{and} \quad \sigma_1, \dots, \sigma_\ell \in \{0,1\}.
$$

3. For all nonempty $I \subseteq \{1, \dots, \ell\}$, compute:
$$
r_I = \bigoplus_{i \in I} s_i, \quad \sigma_I = \bigoplus_{i \in I} \sigma_i
$$

4. For each bit $i = 1, \dots, n$, compute:
$$
x_i = \mathrm{majority}_I \{ \sigma_I \oplus A(y, r_I \oplus e_i) \}
$$

5. Output 
$$
x = x_1 x_2 \dots x_n
$$

Assume $y = f(\hat{x})$, $\hat{x} \in S_n$, and $\sigma_i = gl(\hat{x}, s_i)$ for all $i$. Then $\sigma_I = gl(\hat{x}, r_I)$ and

$$
x_i = \mathrm{majority}_I \{ \sigma_I \oplus A(y, r_I \oplus e_i) \}.
$$

For each $i$ and nonempty $I$:

$$
\Pr[x_I^i = \hat{x}_i] \ge \frac12 + \frac{\varepsilon(n)}{2}, \quad \text{independent over } I.
$$

By Chernoff/Prop A.13:

$$
\Pr[x_i \neq \hat{x}_i] \le \frac{1}{2n} \implies \Pr[x = \hat{x}] \ge \frac12.
$$

Including success probabilities of $\hat{x} \in S_n$ and correct $\sigma_i$:

$$
\Pr[A'\text{ inverts } y] \ge \frac{\varepsilon(n)^3}{20 n} = \frac{1}{20\, n\, p(n)^3}.
$$
___


>**Theorem.** Goldreich-Levin
If there exists a function $f: \{0,1\}^* \to \{0,1\}^*$ which is one-way, then $$gl(x,r) = \langle x, r \rangle \mod 2 = \bigoplus\limits_{i=1}^n x_ir_n$$
must be a hard-core predicate of
$${g(x,r)=(f(x),r)}$$
where ${x,r \in {0,1}^n}$ and $r$ is uniformly chosen.

Recall:
>**Theorem.** One-way function
>A function $f: \{0,1\}^n \to \{0,1\}^*$ is a one-way function if it is efficiently computable and if for every PPT adversary $A$, there exists some negligible function $negl(n)$ such that
>$$\Pr\limits_{x \in \{0,1\}^n}[A(1^n, f(x)) \in f^{-1}(f(x))] \le negl(n)$$

>**Theorem.** Hard-core predicate
>A function $hc(x)$ is a hard-core predicate if for every PPT adversary A, there exists $negl(n)$ such that
>$$\Pr\limits_{x \in \{0,1\}^n}[A(1^n, f(x)) = hc(x)] \le negl(n)$$

*Proof:*

First consider **simple case**.
Suppose exists PPT adversary $A$ where 
$$\Pr\limits_{x,r\in\{0,1\}^n}[A(f(x),r) = gl(x,r)] = 1$$
In other words, $A$ can predict the hardcore bit with perfect accuracy.

Then we can construct reduction $B$ which takes $1^n$ and $y$ and tries to compute the inverse. Suppose $y = f(\hat{x})$.

For $i \in 1,2,...,n$
$B$ solves $x_i = A(f(\hat{x}),e_i) = gl(\hat{x}, e_i) = \hat{x}_i$
where $\hat{x}$ is the original input to the one way function
and outputs $x = x_1,...,x_n$

Notice that since $x_i = \hat{x}_i$, 
$$\Pr[B(1^n,f(x)) \in f^{-1}(f(x)) = 1]$$
This means the reduction $B$ *always* inverts successfully (defined as outputting in the preimage)

___

Next, consider **more involved case**
Suppose exists PPT adversary $A$ where 
$$\Pr\limits_{x,r\in\{0,1\}^n}[A(f(x),r) = gl(x,r)] = 3/4 + 1/p(n)$$
for infinitely many $n$

First consider how we are going to invert.
We can construct reduction $B$ defined as
For $i = 1,2,...,n$
Compute $A(x,r) \oplus A(x,r \oplus e^i)$
Define $x_i$ to be the majority vote of the above calculation
Output $x = x_1,...,x_n$

>[!question] Why does the original construction not work?
>For the inversion to succeed, every bit $x_i$ has to be right. Since we can't guarantee two bits $x_i$ and $x_j$ are independent from each other, the failure probability can only be linearly bounded with union bound (e.g. $1 - n(1/4 - \epsilon)$) which is worthless for large $n$. Thus, we have to introduce the mechanism of majority vote, amplifying the success probability per bit by running it multiple times. We can use an error correcting code and apply Chernoff bound principles.


1. We can prove there are $|S_n| = 2^{n-1}/p(n)$ "good" $x$'s which have at least $3/4 + \frac{1}{2p(n)}$ prediction success rate. Note that this means there is a $\frac{2^{n-1}/p(n)}{2^n} = \frac{1}{2p(n)}$ chance any arbitrary $x \in \{0,1\}^n$ is in $S_n$
2. We can prove by union bound, the probability of inverting a single bit $x_i$ if its "good" (in other words $x \in S_n$) is at least $1 - (1/2 - \frac{1}{p(n)}) = 1/2 + \frac{1}{p(n)}$. That's just over $1/2$, so we can amplify the success probability using repeated queries, which we do polynomial times due to Chernoff bound.

With these two, we find
$$\Pr[B(1^n,x) \in f^{-1}f(x) | x \in S_n ]\Pr[x\in S_n] \ge \frac{1}{4p(n)}$$

So the inversion probability
$$\Pr\limits_{x\in\{0,1\}^n}[B(1^n,x) \in f^{-1}f(x)] \ge \frac{1}{4p(n)}$$

___

**Full proof**
Suppose exists PPT adversary $A$ where 
$$\Pr\limits_{x,r\in\{0,1\}^n}[A(f(x),r) = gl(x,r)] = 1/2 + 1/p(n)$$
for infinitely many $n$

Similar to above, we can show there are $|S_n| = 2^{n-1}/p(n)$ "good" $x$'s which have at least $1/2 + \frac{1}{2p(n)}$ prediction success rate.

We can construct reduction $B$, which takes input $1^n$ and $y$ and tries to compute the inverse of $y$
1. Take sets $s^1, s^2, ..., s^l \in \{0,1\}^n$ and $\sigma^0, ..., \sigma^l \in \{0,1\}$
2. Compute for any nonempty set $I = \{1,...,l\}$, compute $r^I = \bigoplus\limits_{i=1}^ls^i$ and $\sigma^I = \bigoplus\limits_{i=1}^l\sigma^i$

3. For all $i = 1,2,...n$, $l = \lceil log(2n/\epsilon(n)) + 1 \rceil$
Calculate $\sigma^I \oplus A(y, r^I \oplus e^I)$ and output majority vote as $x_i$
4. Output $x = x_1,...,x_n$

>[!question] Why not keep our earlier construction?
>Our earlier construction relies on two components, $A(y, r)$ and $A(y, r \oplus e^I)$ being correct. Since after applying union bound, the success rate of inverting a bit was just over $1/2$, the success was amplifiable. But since in this case, $1/2 + 1/p(n)$ isn't enough, this would be negligible.
> 
> Instead, we simply randomly guess $A(y,r)$ at a logarithm to $n$ rate amount so that this probability ends up being non-negligible ($\frac{1}{2^l} \ge 2n / \epsilon(n)$). For $A(y, r \oplus e^I)$, instead of querying $r$ at total uniform random from $\{0,1\}^n$, we take advantage of the fact that any unique nonempty subsets of $\{0,1\}^n$ are pairwise independent with each other (because there's at least one element in one subset not in the other). By querying $r$ based off these subsets, we can successfully amplify the probability with a weaker bound then the Chernoff bound based off the Chebyshev inequality (roughly $O(n^2)$).
> 
> This sets a balance between making sure the guess rate of $A(y,r)$ is non-negligible while amplifying the success rate of guessing $A(y,r \oplus e^I)$ enough.